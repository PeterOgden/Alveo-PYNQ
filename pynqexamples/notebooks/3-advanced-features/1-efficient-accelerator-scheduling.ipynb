{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Embedded Scheduler\n",
    "\n",
    "Most Alveo shells include a processor on the card to perform low-latency scheduling of accelerators - the _embedded runtime_ or ERT. PYNQ provides a dedicated `start_ert` function that has the same API as `start` but makes use of the scheduler to allow multiple kernel executions to be queued.\n",
    "\n",
    "To show this we are going to use the `mmult` and `vadd` kernels in the advanced designs bitstream to perform a 2x2 tiled matrix multiplication. The core matrix kernel performs a fixed 512 x 512 multiplication so the final result will be a 1024 x 1024 matrix multiplication. As this operation requires 8 invocations of the `mmult` kernel and 4 invocations of the `vadd` with some data-dependencies between them it provides a good example for performing this type of work.\n",
    "\n",
    "The rest of this notebook is split into 3 sections: first we'll map out the algorithm and implement in software; next we'll use the standard `call` and `start` functions to perform the multiplication in hardware; and finally we'll use the embedded scheduler and see how much the performance increases.\n",
    "\n",
    "## Setting up the data structures\n",
    "\n",
    "First we need to create our test matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "KERNEL_SIZE = 512\n",
    "KERNEL_SHAPE = (KERNEL_SIZE, KERNEL_SIZE)\n",
    "MAT_SHAPE = (KERNEL_SIZE * 2, KERNEL_SIZE * 2)\n",
    "\n",
    "in_a = np.random.randint(100, size=MAT_SHAPE, dtype='i4')\n",
    "in_b = np.random.randint(100, size=MAT_SHAPE, dtype='i4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the tiled versions of them that we can use with the accelerator. A more complex design could perform the tiling in hardware to avoid this step but this is sufficient for our purposes. We use the PYNQ `allocate` function so that the buffers are ready for use with the hardware later. We can use the numpy `transpose` function to get our tiles laid out correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tiles_a = np.ndarray(shape=(2, 2, KERNEL_SIZE, KERNEL_SIZE), dtype='i4')\n",
    "in_tiles_b = np.ndarray(shape=(2, 2, KERNEL_SIZE, KERNEL_SIZE), dtype='i4')\n",
    "\n",
    "in_tiles_a[:] = np.transpose(in_a.reshape(2, KERNEL_SIZE, 2, KERNEL_SIZE), (0,2,1,3))\n",
    "in_tiles_b[:] = np.transpose(in_b.reshape(2, KERNEL_SIZE, 2, KERNEL_SIZE), (0,2,1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to need some space for temporarily storing the output of each tiled multiplication and the buffer for the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_buf = np.ndarray(shape=(2, KERNEL_SIZE, KERNEL_SIZE), dtype='i4')\n",
    "out_tiles = np.ndarray(shape=(2,2, KERNEL_SIZE, KERNEL_SIZE), dtype='i4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tiled algorithm in software\n",
    "\n",
    "\n",
    "Before we start running in hardware we want to make sure that our tiling algorithm runs in software correctly. The core function is just two nested loops with the inner loop unrolled to make the accumulation more obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        temp_buf[0] = in_tiles_a[i,0] @ in_tiles_b[0,j]\n",
    "        temp_buf[1] = in_tiles_a[i,1] @ in_tiles_b[1,j]\n",
    "        out_tiles[i,j] = temp_buf[0] + temp_buf[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the result we can undo the transpose and check to make sure that result matches a plain matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = out_tiles.transpose(0,2,1,3).reshape(2*KERNEL_SIZE,2*KERNEL_SIZE)\n",
    "np.array_equal(in_a @ in_b, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving to hardware\n",
    "\n",
    "Now we've validated the concept we can try running our hardware. The bitstream we are going to use has two cores - a matrix multiplication core and vector addition core - both hard coded for 512x512 matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pynq\n",
    "ol = pynq.Overlay('advanced.xclbin')\n",
    "\n",
    "mmult = ol.mmult_1\n",
    "vadd = ol.vadd_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create new input data and create our buffers using `pynq.allocate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_a = np.random.randint(100, size=MAT_SHAPE, dtype='i4')\n",
    "in_b = np.random.randint(100, size=MAT_SHAPE, dtype='i4')\n",
    "\n",
    "in_tiles_a = pynq.allocate(shape=(2, 2, KERNEL_SIZE, KERNEL_SIZE), dtype='i4')\n",
    "in_tiles_b = pynq.allocate(shape=(2, 2, KERNEL_SIZE, KERNEL_SIZE), dtype='i4')\n",
    "temp_buf = pynq.allocate(shape=(2, 2, KERNEL_SIZE, KERNEL_SIZE), dtype='i4')\n",
    "out_tiles = pynq.allocate(shape=(2,2, KERNEL_SIZE, KERNEL_SIZE), dtype='i4')\n",
    "\n",
    "in_tiles_a[:] = np.transpose(in_a.reshape(2, KERNEL_SIZE, 2, KERNEL_SIZE), (0,2,1,3))\n",
    "in_tiles_b[:] = np.transpose(in_b.reshape(2, KERNEL_SIZE, 2, KERNEL_SIZE), (0,2,1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we flush the input buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tiles_a.flush()\n",
    "in_tiles_b.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loop looks identical to the software version execpt we are calling the accelerators rather than using the numpy operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        mmult.call(in_tiles_a[i,0], in_tiles_b[0,j], temp_buf[0])\n",
    "        mmult.call(in_tiles_a[i,1], in_tiles_b[1,j], temp_buf[1])\n",
    "        vadd.call(temp_buf[0], temp_buf[1], out_tiles[i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to retrieve the output tiles from the PCIe card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tiles.invalidate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check the result with software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(out_tiles.transpose(0,2,1,3).reshape((1024,1024)), in_a @ in_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlapping Execution\n",
    "\n",
    "In the previous implementation each accelerator execution is synchronous. As the mmult and vadd kernel are completely separate we should be able to overlap them. Unfortunately manually expanding the loops to allow for overlapping is tricky and potentially error prone. Instead we can use the ERT to handle the dataflow graph for us.\n",
    "\n",
    "First we need to create a bigger temporary buffer so that we can run two iterations simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_buf = pynq.allocate(shape=(4, KERNEL_SIZE, KERNEL_SIZE), dtype='i4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can execute our previous loop using `start_ert` functions using the `waitlist` parameter to enforce the data-dependence. We add an additional dependency between the vadd iterations to ensure that when the final loop iteration is waited on we know the computation is finished. Note that `None` is a valid handle to pass and can be ignored making our inter-loop dependency simpler to express."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_wh = None\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        wh1 = mmult.start_ert(in_tiles_a[i,0], in_tiles_b[0,j], temp_buf[0])\n",
    "        wh2 = mmult.start_ert(in_tiles_a[i,1], in_tiles_b[1,j], temp_buf[1])\n",
    "        sum_wh = vadd.start_ert(temp_buf[0 + 2*j], temp_buf[1 + 2*j], out_tiles[i,j], waitlist=(wh1, wh2, sum_wh))\n",
    "sum_wh.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the relative performance improvement using the `%%timeit` magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.9 ms ± 3.56 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        mmult.call(in_tiles_a[i,0], in_tiles_b[0,j], temp_buf[0])\n",
    "        mmult.call(in_tiles_a[i,1], in_tiles_b[1,j], temp_buf[1])\n",
    "        vadd.call(temp_buf[0], temp_buf[1], out_tiles[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.7 ms ± 21.1 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "sum_wh = None\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        wh1 = mmult.start_ert(in_tiles_a[i,0], in_tiles_b[0,j], temp_buf[0])\n",
    "        wh2 = mmult.start_ert(in_tiles_a[i,1], in_tiles_b[1,j], temp_buf[1])\n",
    "        sum_wh = vadd.start_ert(temp_buf[0 + 2*j], temp_buf[1 + 2*j], out_tiles[i,j], waitlist=(wh1, wh2, sum_wh))\n",
    "sum_wh.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple modification to our code to use the embedded scheduler has resulted in a 15% reduction of execution time.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "Finally we need to free the resources to let other processes use the FPGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%xdel in_tiles_a\n",
    "%xdel in_tiles_b\n",
    "%xdel temp_buf\n",
    "%xdel out_tiles\n",
    "ol.free()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
